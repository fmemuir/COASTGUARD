{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e9e93a92-9e90-408c-88a2-d49c1c53c074",
   "metadata": {},
   "source": [
    "### Import necessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "355c8ba7-6902-48b4-9698-c9b367436e0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-14 10:47:26.615106: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-08-14 10:47:37.921108: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "# from sklearn.utils.class_weight import compute_class_weight\n",
    "# from imblearn.over_sampling import SMOTE\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras import Input\n",
    "from tensorflow.keras.layers import GRU, Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from tensorflow.keras.callbacks import EarlyStopping, TensorBoard\n",
    "\n",
    "from Toolshed import Predictions\n",
    "\n",
    "# Only use tensorflow in CPU mode\n",
    "import tensorflow as tf\n",
    "tf.config.set_visible_devices([],'GPU')\n",
    "%load_ext tensorboard\n",
    "import tensorboard\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dac3dcc4-aff3-4afa-826d-8e62c2c74832",
   "metadata": {},
   "source": [
    "### Define site variables and set up directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78cf828f-977a-44e8-8563-3906b17b29f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Name of site to save directory and files under\n",
    "sitename = 'StAndrewsEastS2Full'\n",
    "filepath = os.path.join(os.getcwd(), 'Data')\n",
    "\n",
    "# Load in transect data with coastal change variables\n",
    "TransectInterGDF, TransectInterGDFWater, TransectInterGDFTopo, TransectInterGDFWave = Predictions.LoadIntersections(filepath, sitename)\n",
    "# Compile relevant coastal change metrics into one dataframe\n",
    "CoastalDF = Predictions.CompileTransectData(TransectInterGDF, TransectInterGDFWater, TransectInterGDFTopo, TransectInterGDFWave)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5489a438-c34d-4bed-a4ac-82b965b27629",
   "metadata": {},
   "source": [
    "### Prepare data for training\n",
    "Ensure each of the variables are synchronised by timestep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e9f6b792-d434-4500-8ccb-84c651d2a3e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    TransectID                                              dates  \\\n",
      "50          50  [2015-08-07, 2015-08-17, 2015-11-22, 2016-02-1...   \n",
      "\n",
      "                                            distances  \\\n",
      "50  [137.069698951171, 144.04709852809864, 138.324...   \n",
      "\n",
      "                                           wlcorrdist  \\\n",
      "50  [509.7608836456035, 181.57936499534543, 440.88...   \n",
      "\n",
      "                                            waterelev  \\\n",
      "50  [-2.079662590257345, 0.5471072274658595, -1.75...   \n",
      "\n",
      "                                           beachwidth  \\\n",
      "50  [365.7137851175048, 43.25529176978043, 295.990...   \n",
      "\n",
      "                                              TZwidth  TZwidthMn   SlopeMax  \\\n",
      "50  [10.196233442125981, 10.196233442125981, 10.19...  15.273152  29.881193   \n",
      "\n",
      "    SlopeMean                                             WaveHs  \\\n",
      "50  10.838753  [0.25638099458485064, 0.16007945575151394, 0.8...   \n",
      "\n",
      "                                              WaveDir  \\\n",
      "50  [91.71905048709071, 70.2255225279233, 57.96026...   \n",
      "\n",
      "                                               WaveTp  WaveDiffus  \n",
      "50  [5.599713980392192, 5.104121052580668, 5.86980...   -0.000057  \n"
     ]
    }
   ],
   "source": [
    "TransectIDs = [50]\n",
    "\n",
    "for Tr in TransectIDs:\n",
    "    TransectDF = CoastalDF.iloc[[Tr],:] # single-row dataframe\n",
    "print(TransectDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fd41402-a8ab-4639-b6bc-a8f47b05b953",
   "metadata": {},
   "source": [
    "### Feature engineering\n",
    "Time-lag the features to capture temporal dependencies and aggregate features over a moving window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8a8f580-281c-4051-bee4-e34cb3233708",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "268804d0-5345-43be-a85f-dcd23b5f69dd",
   "metadata": {},
   "source": [
    "### Define model\n",
    "\n",
    "RNN model may need attention layer to help focus on important parts of timeseries (i.e. storm/high-risk events)\n",
    "Model architecture:\n",
    "- Input layers: \n",
    "  - Historic obervations layer [shape = (batch_size, timesteps, num of variables)]\n",
    "  - Forecast data layer [shape = (batch_size, forecast_steps, num of variables)]\n",
    "- Feature fusion layer:\n",
    "  - Concatenate two LSTMs into one single feature vector\n",
    "  - Pass to fully connected dense layer\n",
    "- Output layer:\n",
    "  - Softmax layer for generating multiclass of risk levels\n",
    "  - Define model by compiling (using Adam optimiser and categorical cross-entropy loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f0e4006-12c9-487f-b836-30ff93acafc5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0082ac98-ca17-458b-8f6d-fcb780eca70d",
   "metadata": {},
   "source": [
    "### Separate data into train/validate/test data\n",
    "Goal is to minimise differnece between predicted and actually labelled risk levels, which are based on past/current state and strength of forecast severity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef0b4a97-368f-415a-96ee-c99c9a90e4b1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "95e5bcaa-aeef-44c0-962f-beca819e7934",
   "metadata": {},
   "source": [
    "### Prediction\n",
    "Incorporate in forecast data so that newest forecast will guide the overall risk classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47be72b2-af5b-46fb-bc17-a0e64a00763f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
